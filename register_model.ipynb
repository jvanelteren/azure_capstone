{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1598275788035
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Workspace name: capstone\nAzure region: westeurope\nSubscription id: 72f46e0e-1451-4b79-92cd-fc8f7797bda7\nResource group: test-Jesse\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment, Model\n",
    "import joblib\n",
    "\n",
    "# ws = Workspace.get(name=\"capstone\")\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# exp = Experiment(workspace=ws, name=\"udacity-project-hyperparam-bayes\")\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1598275788675
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# from https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hyperdrive xgboost with bandit policy\t0.1259\n",
      "hyperdrive xgboost with bayesian sampling\t0.127\n",
      "AutoML on same dataset as hyperdrive\t0.1167\n",
      "AutoML on original dataset (except log transform target)\t0.1248\n",
      "AutoML on original dataset (no log transform target)\t26738.5013\n"
     ]
    }
   ],
   "source": [
    "# I've ran five experiments, \n",
    "explanation = { 'udacity-project-hyperparam-bandit' : 'hyperdrive xgboost with bandit policy',\n",
    "                'udacity-project-hyperparam-bayes' : 'hyperdrive xgboost with bayesian sampling',\n",
    "                'automl': 'AutoML on same dataset as hyperdrive',\n",
    "                'AutoML-onlylogtransform': 'AutoML on original dataset (except log transform target)',\n",
    "                'automl-kaggle-orgininal': 'AutoML on original dataset (no log transform target)'\n",
    "}\n",
    "\n",
    "def get_score(run):\n",
    "    print(float(run.get_metrics()['rmse']) if run.status == 'Completed' else None)\n",
    "    return float(run.get_metrics()['rmse']) if run.status == 'Completed' else None\n",
    "\n",
    "def get_main_run(exp_name):\n",
    "    exp = Experiment(workspace=ws, name=exp_name)\n",
    "    return list(exp.get_runs())[0]\n",
    "\n",
    "def get_best_run(name, run):\n",
    "    print(explanation[name], end = '\\t')\n",
    "    if run.type == 'hyperdrive':\n",
    "        best_run = run.get_best_run_by_primary_metric()\n",
    "        print(round(best_run.get_metrics()['rmse'],4))\n",
    "    elif run.type == 'automl':\n",
    "        best_run = run.get_best_child()\n",
    "        print(round(best_run.get_metrics()['root_mean_squared_error'],4))\n",
    "    else:\n",
    "        print('wrong type')\n",
    "    return best_run\n",
    "\n",
    "main_runs = {exp_name : get_main_run(exp_name) for exp_name in list(explanation.keys())}\n",
    "best_runs = {name: get_best_run(name, run) for name, run in main_runs.items()}\n",
    "    "
   ]
  },
  {
   "source": [
    "## Register model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = main_runs['automl'].register_model(model_name = 'automl2')\n",
    "model.download(target_dir='models/automl2', exist_ok = True)\n",
    "model = joblib.load('models/automl/outputs/model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, best_model = main_runs['automl'].get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='capstone', subscription_id='72f46e0e-1451-4b79-92cd-fc8f7797bda7', resource_group='test-Jesse'), name=automl3, id=automl3:1, version=1, tags={}, properties={})"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "best_run.register_model(model_name = 'automl3', model_path='./outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RegressionPipeline(pipeline=Pipeline(memory=None,\n",
       "                                     steps=[('datatransformer',\n",
       "                                             DataTransformer(enable_dnn=None,\n",
       "                                                             enable_feature_sweeping=None,\n",
       "                                                             feature_sweeping_config=None,\n",
       "                                                             feature_sweeping_timeout=None,\n",
       "                                                             featurization_config=None,\n",
       "                                                             force_text_dnn=None,\n",
       "                                                             is_cross_validation=None,\n",
       "                                                             is_onnx_compatible=None,\n",
       "                                                             logger=None,\n",
       "                                                             observer=None,\n",
       "                                                             task=None,\n",
       "                                                             working_dir=None)),\n",
       "                                            ('pre...\n",
       "                                                                                                                   fit_intercept=True,\n",
       "                                                                                                                   l1_ratio=0.01,\n",
       "                                                                                                                   max_iter=1000,\n",
       "                                                                                                                   normalize=False,\n",
       "                                                                                                                   positive=False,\n",
       "                                                                                                                   precompute=False,\n",
       "                                                                                                                   random_state=None,\n",
       "                                                                                                                   selection='cyclic',\n",
       "                                                                                                                   tol=0.0001,\n",
       "                                                                                                                   warm_start=False))],\n",
       "                                                                                                verbose=False))],\n",
       "                                                                          weights=[0.13333333333333333,\n",
       "                                                                                   0.2,\n",
       "                                                                                   0.26666666666666666,\n",
       "                                                                                   0.06666666666666667,\n",
       "                                                                                   0.06666666666666667,\n",
       "                                                                                   0.06666666666666667,\n",
       "                                                                                   0.06666666666666667,\n",
       "                                                                                   0.13333333333333333]))],\n",
       "                                     verbose=False),\n",
       "                   stddev=None)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "source": [
    "## Get testset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://raw.githubusercontent.com/jvanelteren/housing/master/datasets/housing_after_preprocessing.csv\"\n",
    "train_x = pd.read_csv(train_url)\n",
    "train_x.rename(columns={'Unnamed: 0':'Column1'}, inplace=True)\n",
    "train_x = train_x.drop(columns=['y'])\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "class DFSimpleImputer(SimpleImputer):\n",
    "    # just like SimpleImputer, but retuns a df\n",
    "    # this approach creates problems with the add_indicator=True, since more columns are returned\n",
    "    # so don't set add_indicator to True\n",
    "    def transform(self, X,y=None):\n",
    "        return pd.DataFrame(super().transform(X),columns=X.columns) \n",
    "    def __repr__(self):\n",
    "        return f'SimpleImputer'\n",
    "imp = DFSimpleImputer(strategy='most_frequent')\n",
    "imp = imp.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "float64\n",
      "object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/jvanelteren/housing/master/datasets/final_test.csv\"\n",
    "test_x = pd.read_csv(test_url)\n",
    "test_x.rename(columns={'Unnamed: 0':'Column1'}, inplace=True)\n",
    "test_x = imp.transform(test_x)\n",
    "test_x = test_x.astype(train_x.dtypes.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model, filename='submission.csv'):\n",
    "    pred = model.predict(test_x)\n",
    "    out = pd.DataFrame()\n",
    "    out['Id']= list(range(1461, 2920))\n",
    "    out['SalePrice'] = np.exp(pred)\n",
    "    out.to_csv('./submissions/'+ filename, index=False)\n",
    "submit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from azureml.core.run import Run\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "train_url = [\"https://raw.githubusercontent.com/jvanelteren/housing/master/datasets/housing_after_preprocessing.csv\"]\n",
    "train_x = TabularDatasetFactory.from_delimited_files(train_url).drop_columns('y')\n",
    "train_y = TabularDatasetFactory.from_delimited_files(train_url).keep_columns('y')\n",
    "\n",
    "test_url = [\"https://raw.githubusercontent.com/jvanelteren/housing/master/datasets/final_test.csv\"]\n",
    "test_x = TabularDatasetFactory.from_delimited_files(test_url)\n",
    "\n",
    "train_x = train_x.to_pandas_dataframe()\n",
    "train_y = train_y.to_pandas_dataframe()\n",
    "test_x = test_x.to_pandas_dataframe()\n",
    "\n",
    "train_x = train_x.fillna(value=np.nan)\n",
    "test_x = train_x.fillna(value=np.nan)\n",
    "\n",
    "\n",
    "# def get_pipeline(impute_cat='DFSIMPLEIMPUTER', impute_num =DFSIMPLEIMPUTER', scale=DFMINMAX',onehot='default',remove_outliers='default'):\n",
    "class DFGetDummies(TransformerMixin):\n",
    "    # actually this should be identical to sklearn OneHotEncoder()\n",
    "    def fit(self, X, y=None):\n",
    "        self.train = pd.get_dummies(X)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        self.test = pd.get_dummies(X)\n",
    "        return self.test.reindex(columns=self.train.columns,fill_value=0)\n",
    "    def __repr__(self):\n",
    "        return 'DFGetDummies'\n",
    "\n",
    "class DFSimpleImputer(SimpleImputer):\n",
    "    # just like SimpleImputer, but retuns a df\n",
    "    # this approach creates problems with the add_indicator=True, since more columns are returned\n",
    "    # so don't set add_indicator to True\n",
    "    def transform(self, X,y=None):\n",
    "        return pd.DataFrame(super().transform(X),columns=X.columns) \n",
    "    def __repr__(self):\n",
    "        return f'SimpleImputer'\n",
    "\n",
    "class DFMinMaxScaler(MinMaxScaler):\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.DataFrame(super().transform(X),columns=X.columns)\n",
    "    def __repr__(self):\n",
    "        return 'DFMinMaxScaler'\n",
    "\n",
    "class DFColumnTransformer(ColumnTransformer):\n",
    "    # works only with non-sparse matrices!\n",
    "    def _hstack(self, Xs):\n",
    "        Xs = [f for f in Xs]\n",
    "        cols = [col for f in Xs for col in f.columns]\n",
    "        df = pd.DataFrame(np.hstack(Xs), columns=cols)\n",
    "        # print('final shape',df.shape)\n",
    "        return df.infer_objects()\n",
    "#%%\n",
    "def get_pipeline():\n",
    "    # in essence this splits the input into a categorical pipeline and a numeric pipeline\n",
    "    # merged with a ColumnTransformer\n",
    "\n",
    "    cat_steps = []\n",
    "    cat_steps.append(('impute_cat', DFSimpleImputer(strategy='most_frequent')))\n",
    "    cat_steps.append(('cat_to_num', DFGetDummies()))\n",
    "    categorical_transformer = Pipeline(steps=cat_steps)\n",
    "\n",
    "    num_steps = []\n",
    "    num_steps.append(('impute_num', DFSimpleImputer(strategy='most_frequent')))\n",
    "    # num_steps.append(('scale_num', DFMinMaxScaler()))\n",
    "    numeric_transformer = Pipeline(steps=num_steps)\n",
    "\n",
    "    col_trans = DFColumnTransformer(transformers=[\n",
    "        ('numeric', numeric_transformer, make_column_selector(dtype_include=np.number)),\n",
    "        ('category', categorical_transformer, make_column_selector(dtype_exclude=np.number)),\n",
    "        ])\n",
    "\n",
    "    preprocessor_steps = [('col_trans', col_trans)]\n",
    "    preprocessor = Pipeline(steps=preprocessor_steps)\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "#%%\n",
    "pipe = get_pipeline()\n",
    "train_x = pipe.fit_transform(train_x, train_y)\n",
    "test_x = pipe.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModelNotFoundException",
     "evalue": "ModelNotFoundException:\n\tMessage: Model /outputs/model.pkl not found in cache at azureml-models or in current working directory d:\\Documenten\\GitHub\\courses\\Nanodegree Azure Machine Learning\\Azure ML Capstone. For more info, set logging level to DEBUG.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Model /outputs/model.pkl not found in cache at azureml-models or in current working directory d:\\\\Documenten\\\\GitHub\\\\courses\\\\Nanodegree Azure Machine Learning\\\\Azure ML Capstone. For more info, set logging level to DEBUG.\"\n    }\n}",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelNotFoundException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-78e7a53ad2f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/outputs/model.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\azureml\\core\\model.py\u001b[0m in \u001b[0;36mget_model_path\u001b[1;34m(model_name, version, _workspace)\u001b[0m\n\u001b[0;32m    803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model_path_remote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactive_workspace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model_path_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\azureml\\core\\model.py\u001b[0m in \u001b[0;36m_get_model_path_local\u001b[1;34m(model_name, version)\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;31m# Probing azureml-models/<name>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model_path_local_from_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[1;31m# Probing azureml-models/<name> exists, probing version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\azureml\\core\\model.py\u001b[0m in \u001b[0;36m_get_model_path_local_from_root\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    868\u001b[0m         raise ModelNotFoundException(\"Model {} not found in cache at {} or in current working directory {}. \"\n\u001b[0;32m    869\u001b[0m                                      \"For more info, set logging level to DEBUG.\".format(model_name, MODELS_DIR,\n\u001b[1;32m--> 870\u001b[1;33m                                                                                          os.getcwd()))\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModelNotFoundException\u001b[0m: ModelNotFoundException:\n\tMessage: Model /outputs/model.pkl not found in cache at azureml-models or in current working directory d:\\Documenten\\GitHub\\courses\\Nanodegree Azure Machine Learning\\Azure ML Capstone. For more info, set logging level to DEBUG.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Model /outputs/model.pkl not found in cache at azureml-models or in current working directory d:\\\\Documenten\\\\GitHub\\\\courses\\\\Nanodegree Azure Machine Learning\\\\Azure ML Capstone. For more info, set logging level to DEBUG.\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Get your best run and save the model from that run.\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])\n",
    "print(best_run_metrics['Accuracy'])\n",
    "files = best_run.get_file_names()\n",
    "best_run.download_file(files[-1], output_file_path='./outputs/')\n",
    "joblib.load('./outputs/model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AutoML on same dataset as hyperdrive\t"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Run' object has no attribute 'get_best_child'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-db5ea02338c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rmse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'automl'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mbest_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'root_mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# best_run.register_model(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Run' object has no attribute 'get_best_child'"
     ]
    }
   ],
   "source": [
    "exp_name = all_exp[2]\n",
    "print(explanation[exp_name], end='\\t')\n",
    "exp = Experiment(workspace=ws, name=exp_name)\n",
    "run = list(exp.get_runs())[0]\n",
    "if run.type == 'hyperdrive':\n",
    "    best_run = run.get_best_run_by_primary_metric()\n",
    "    print(round(best_run.get_metrics()['rmse'],3))\n",
    "elif run.type == 'automl':\n",
    "    best_run = run.get_best_child()\n",
    "    print(round(best_run.get_metrics()['root_mean_squared_error'],3))\n",
    "# best_run.register_model(\n",
    "#    model_name=exp_name, \n",
    "#    model_path=\"outputs/model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}